% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim_mest.R
\name{optim_mest}
\alias{optim_mest}
\title{M-Estimation using Automatic Differentiation}
\usage{
optim_mest(
  psi,
  theta0,
  data,
  ...,
  method = c("lbfgs", "adam"),
  use_autodiff = TRUE,
  control = list(),
  constraints = NULL
)
}
\arguments{
\item{psi}{A function that computes per-observation estimating equations. Must
accept at least two arguments: \code{theta} (parameters) and \code{data}. In torch-native
mode, it should return a tensor of shape (n, p) where n is the number of
observations and p is the number of parameters. In R function mode, it should
return a matrix with n rows and p columns. Each row represents the contribution
from one observation, and the column means should be zero at the solution.}

\item{theta0}{A named numeric vector of starting values for the parameters.}

\item{data}{The data object to be passed to \code{psi}.}

\item{...}{Additional arguments passed to \code{psi}.}

\item{method}{Character string specifying the optimizer to use. Options:
\code{"lbfgs"} (default) or \code{"adam"}.}

\item{use_autodiff}{Logical. If \code{TRUE} (default), automatically detect
torch-native functions and use autograd when possible. If \code{FALSE}, use
finite-difference gradients.}

\item{control}{List of control parameters for optimization (max_iter, tolerance, etc.).}

\item{constraints}{Optional constraint specification. Can be a single constraint
object or a \code{constraints()} list. See \code{optim_mle()} for details.}
}
\value{
An object of class \code{autodiffr_fit} containing:
\item{coefficients}{Named numeric vector of parameter estimates}
\item{convergence}{Convergence code (0 = success)}
\item{message}{Convergence message}
\item{iterations}{Number of iterations}
\item{gradient_norm}{Norm of the final gradient}
\item{gradient}{Named numeric vector of final gradients}
\item{vcov}{Sandwich variance-covariance matrix}
\item{method}{Estimation method ("mest")}
\item{call}{The original function call}
}
\description{
Fit M-estimators by solving estimating equations using automatic differentiation.
This function minimizes the sum of squared mean estimating equations and computes
sandwich (Godambe) variance-covariance matrices.
}
\details{
M-estimation solves estimating equations of the form:
\deqn{\sum_{i=1}^n \psi_i(\theta) = 0}

This function minimizes \eqn{Q(\theta) = 0.5 \sum (\bar{\psi}(\theta))^2} where
\eqn{\bar{\psi}(\theta)} is the mean of the per-observation estimating equations.

The sandwich variance-covariance matrix is computed as:
\deqn{V = J^{-1} S (J^{-1})^T / n}
where \eqn{J = \partial \bar{\psi} / \partial \theta^T} (bread) and
\eqn{S = \text{cov}(\psi_i)} (meat).
}
\examples{
\dontrun{
# Example: Linear regression via M-estimation
library(torch)
set.seed(123)
n <- 100
X <- cbind(1, rnorm(n))
y <- X \%*\% c(2, 3) + rnorm(n)
data_list <- list(X = X, y = y)

# Estimating equations for OLS
psi_ols <- function(theta, data) {
  beta <- theta[1:2]
  residuals <- data$y - data$X \%*\% beta
  psi_mat <- residuals * data$X
  return(psi_mat)
}

start <- c(beta0 = 0, beta1 = 0)
fit <- optim_mest(psi_ols, start, data_list)
print(fit)
}
}
