% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/check_grad.R
\name{check_grad}
\alias{check_grad}
\title{Check gradients computed by autograd}
\usage{
check_grad(loglik, theta0, data, ..., eps = 1e-06, rel_tol = 0.001)
}
\arguments{
\item{loglik}{A torch-native function that computes the log-likelihood. Must
accept torch tensors as the first argument and return a scalar torch tensor.}

\item{theta0}{Named numeric vector of parameter values at which to check gradients.}

\item{data}{The data object to be passed to \code{loglik}.}

\item{...}{Additional arguments passed to \code{loglik}.}

\item{eps}{Step size for finite differences (default: 1e-6).}

\item{rel_tol}{Relative tolerance for considering gradients acceptable
(default: 1e-3).}
}
\value{
An object of class \code{autodiffr_gradcheck} containing:
\item{theta}{The parameter values at which gradients were checked}
\item{grad_autograd}{Gradients computed via autograd}
\item{grad_fd}{Gradients computed via finite differences}
\item{abs_err}{Absolute errors between autograd and finite-difference gradients}
\item{rel_err}{Relative errors between autograd and finite-difference gradients}
\item{max_abs_err}{Maximum absolute error}
\item{max_rel_err}{Maximum relative error}
\item{ok}{Logical indicating if all relative errors are within tolerance}
}
\description{
Compares autograd gradients with finite differences to make sure your
log-likelihood function is implemented correctly. Useful for debugging
when things aren't working as expected.
}
\examples{
\dontrun{
library(torch)
set.seed(123)
data <- rnorm(100, mean = 5, sd = 2)
data_tensor <- torch_tensor(data, dtype = torch_float64())

# Torch-native loglikelihood
loglik_torch <- function(theta, data) {
  mu <- theta[1]
  sigma <- torch::torch_clamp(theta[2], min = 1e-6)
  dist <- torch::distr_normal(mu, sigma)
  dist$log_prob(data)$sum()
}

theta0 <- c(mu = 5, sigma = 2)
check <- check_grad(loglik_torch, theta0, data_tensor)
print(check)
}
}
