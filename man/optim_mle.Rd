% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim_mle.R
\name{optim_mle}
\alias{optim_mle}
\title{Maximum Likelihood Estimation using Automatic Differentiation}
\usage{
optim_mle(
  loglik,
  start,
  data,
  optimizer = "lbfgs",
  max_iter = 1000,
  tolerance = 1e-06,
  use_fallback = FALSE,
  constraints = NULL,
  ...
)
}
\arguments{
\item{loglik}{A function that computes the log-likelihood. In torch-native mode,
it must accept torch tensors as the first argument and return a torch tensor.
In R function mode, it accepts a named numeric vector and returns a scalar.
Must accept at least two arguments: \code{theta} (parameters) and \code{data}.}

\item{start}{A named numeric vector of starting values for the parameters.}

\item{data}{The data object to be passed to \code{loglik}.}

\item{optimizer}{Character string specifying the optimizer to use. Options:
\code{"lbfgs"} (default) or \code{"adam"}.}

\item{max_iter}{Maximum number of iterations (default: 1000).}

\item{tolerance}{Convergence tolerance (default: 1e-6).}

\item{use_fallback}{Logical. If \code{TRUE}, force use of finite-difference gradients
even for torch-native functions. If \code{FALSE} (default), automatically detect
torch-native functions and use autograd when possible.}

\item{constraints}{Optional constraint specification. Can be a single constraint
object (from \code{positive()}, \code{simplex()}, etc.) or a \code{constraints()} list.
When constraints are provided, parameters are transformed to unconstrained
space for optimization, and the log-likelihood is adjusted by the log-Jacobian
of the transformation. The final estimates are returned in the constrained space.}

\item{...}{Additional arguments passed to the optimizer.}
}
\value{
An object of class \code{autodiffr_fit} containing:
\item{coefficients}{Named numeric vector of parameter estimates}
\item{loglik}{Final log-likelihood value}
\item{convergence}{Convergence code (0 = success)}
\item{message}{Convergence message}
\item{iterations}{Number of iterations}
\item{gradient_norm}{Norm of the final gradient}
\item{gradient}{Named numeric vector of final gradients}
\item{vcov}{Variance-covariance matrix (inverse of negative Hessian)}
\item{optimizer}{Character string naming the optimizer used}
\item{call}{The original function call}
}
\description{
Maximize a user-supplied log-likelihood function using automatic differentiation
via the torch package. This function supports two modes:
}
\details{
\strong{Torch-native mode (recommended):} For true automatic differentiation,
provide a \code{loglik} function that accepts torch tensors as the first argument
and uses torch operations internally. This enables exact gradient computation
via autograd.

\strong{R function mode (fallback):} If you provide a standard R function that
accepts numeric vectors, gradients will be computed using finite differences.
This is less accurate and slower, but allows using existing R code.
}
\examples{
\dontrun{
# Example: MLE for normal distribution using torch-native function
library(torch)
set.seed(123)
data <- rnorm(100, mean = 5, sd = 2)
data_tensor <- torch_tensor(data, dtype = torch_float64())

# Torch-native loglikelihood (uses autograd)
loglik_torch <- function(theta, data) {
  mu <- theta[1]
  sigma <- torch::torch_clamp(theta[2], min = 1e-6)  # Ensure positive
  dist <- torch::distr_normal(mu, sigma)
  dist$log_prob(data)$sum()
}

start <- c(mu = 0, sigma = 1)
fit <- optim_mle(loglik_torch, start, data_tensor)
print(fit)

# R function mode (fallback, uses finite differences)
loglik_r <- function(theta, data) {
  mu <- theta["mu"]
  sigma <- theta["sigma"]
  sum(dnorm(data, mean = mu, sd = sigma, log = TRUE))
}

fit2 <- optim_mle(loglik_r, start, data)
print(fit2)
}
}
